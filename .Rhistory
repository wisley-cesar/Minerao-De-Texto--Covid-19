extrair_textos_cloroquina <- function(url) {
# Faz o pedido HTTP para a página
resposta <- read_html(url)
# Verifica se a resposta contém conteúdo
if (length(resposta) > 0) {
# Encontra todos os elementos de texto informativo (por exemplo, parágrafos)
textos_informativos <- html_nodes(resposta, "p")
# Extrai, limpa e armazena todos os textos
textos_todos <- sapply(textos_informativos, function(texto) {
texto_limpo <- limpar_texto(html_text(texto))
return(texto_limpo)
})
return(textos_todos)
} else {
cat("Falha ao acessar a página. A resposta não contém conteúdo.\n")
return(NULL)
}
}
# Função para análise de sentimento em relação à covid
analisar_sentimento_covid <- function(texto) {
# Realiza a análise de sentimento
sentimento <- sentiment(texto)
# Verifica se a palavra "covid" está presente na frase
if ("covid" %in% str_split(texto, " ")[[1]]) {
rotulo <- "Positivo"
} else {
rotulo <- "Negativo"
}
return(rotulo)
}
# Lista de URLs para análise
urls <- c(
"https://diplomatique.org.br/do-bolsodoria-ao-bolsonarovirus-o-discurso-de-joao-doria/",
"https://agenciamural.org.br/panorama-da-covid-19-na-grande-sao-paulo/",
"https://jornal.usp.br/ciencias/bolhas-de-protecao-local-podem-ter-freado-covid-19-em-sao-paulo-aponta-pesquisa/",
"https://vejasp.abril.com.br/saude/sao-paulo-900-mil-casos-covid-19?utm_source=google&utm_medium=cpc&utm_campaign=eda_vejasp_audiencia_institucional&gad_source=1&gclid=Cj0KCQiA6vaqBhCbARIsACF9M6nG3mRC82T1K2T4jKti86xQqVQQW2ToCM5NZhw5ORagaiFKIQzqCuMaAsoBEALw_wcB",
"https://www.bbc.com/portuguese/brasil-51746662",
"https://www.bbc.com/portuguese/brasil-53588285",
"https://brasil.elpais.com/brasil/2020-09-11/sao-paulo-puxa-queda-de-mortes-por-covid-19-no-pais-mas-e-cedo-para-cravar-controle-da-pandemia.html",
"https://www.brasildefato.com.br/2020/06/10/autoridades-estao-mandando-populacao-para-abatedouro-com-reabertura-comercial-em-sp",
"https://www.brasildefato.com.br/2020/06/10/autoridades-estao-mandando-populacao-para-abatedouro-com-reabertura-comercial-em-sp",
"https://www.brasildefato.com.br/2020/07/09/vitimas-da-flexibilizacao-relatos-de-quem-contraiu-covid-na-volta-ao-trabalho-em-sp"
)
# Itera sobre a lista de URLs e aplica o web scraping para cada uma delas
todos_textos <- urls %>%
lapply(extrair_textos_cloroquina) %>%
unlist()
library(rvest)
library(sentimentr)
library(tm)
library(dplyr)
# Lista de stopwords em português
stopwords_pt <- c("de", "a", "o", "que", "e", "é", "do", "da", "em", "um", "para", "com", "não", "uma", "os", "no", "se", "na", "por", "mais", "as", "dos", "como", "mas", "ao", "ele", "das", "à", "seu", "sua", "ou", "quando", "muito", "nos", "já", "eu", "também", "só", "pelo", "pela", "até", "isso", "ela", "entre", "depois", "sem", "mesmo", "aos", "seus", "quem", "nas", "me", "esse", "eles", "você", "essa", "num", "nem", "suas", "meu", "às", "minha", "numa", "pelos", "elas", "qual", "nós", "lhe", "deles", "essas", "esses", "pelas", "este", "dele", "tu", "te", "vocês", "vos", "lhes", "meus", "minhas", "teu", "tua", "teus", "tuas", "nosso", "nossa", "nossos", "nossas", "dela", "delas", "esta", "estes", "estas", "aquele", "aquela", "aqueles", "aquelas", "isto", "aquilo", "estou", "está", "estamos", "estão", "estive", "esteve", "estivemos", "estiveram", "estava", "estávamos", "estavam", "estivera", "estivéramos", "esteja", "estejamos", "estejam", "estivesse", "estivéssemos", "estivessem", "estiver", "estivermos", "estiverem", "hei", "há", "havemos", "hão", "houve", "houvemos", "houveram", "houvera", "houvéramos", "haja", "hajamos", "hajam", "houvesse", "houvéssemos", "houvessem", "houver", "houvermos", "houverem", "houverei", "houverá", "houveremos", "houverão", "houveria", "houveríamos", "houveriam", "sou", "somos", "são", "era", "éramos", "eram", "fui", "foi", "fomos", "foram", "fora", "fôramos", "seja", "sejamos", "sejam", "fosse", "fôssemos", "fossem", "for", "formos", "forem", "serei", "será", "seremos", "serão", "seria", "seríamos", "seriam", "tenho", "tem", "temos", "tém", "tinha", "tínhamos", "tinham", "tive", "teve", "tivemos", "tiveram", "tivera", "tivéramos", "tenha", "tenhamos", "tenham", "tivesse", "tivéssemos", "tivessem", "tiver", "tivermos", "tiverem", "faz")
# Palavras adicionais para remover
palavras_a_remover <- c("saúde", "política", "outrotermo", "maisumtermo")
# Palavras personalizadas para aprimorar a análise
stopwords_personalizadas <- c(
"covid", "pandemia", "saúde", "política", "isolamento", "coronavírus",
"vírus", "governo", "casos", "medidas", "quarentena", "vacin", "vacinação",
"contágio", "público", "privado", "impacto", "social", "econômico",
"pandêmico", "lockdown", "epidemia", "trabalho", "economia", "máscara",
"distanciamento", "números", "infectados", "mortes", "vacinas", "imunização",
"doença", "tratamento", "hospital", "prevenção", "transmissão", "isolamento",
"medicamentos", "sintomas", "pesquisa", "científico", "comunidade", "crise",
"precauções", "impacto", "medidas", "segurança", "isolamento", "flexibilização",
"notícias", "informações", "dados", "cenário", "atualização", "sars-cov-2",
"gráfico", "recuperação", "teste", "testagem", "laboratório", "vacinação",
"variantes", "eficácia", "fronteiras", "população", "unidades", "sintomas",
"transparência", "trabalhadores", "grupos", "sociedade", "impactos", "medidas",
"vacinação", "vacinas", "prevenção", "isolamento", "tratamento", "fases",
"protocolos", "especialistas", "relatório", "estudo", "pesquisa", "monitoramento",
"situação", "protocolos", "isolamento", "transparência", "informações", "notícias",
"opiniões", "especialistas", "opinião", "debate", "avaliação", "perspectiva",
"cenário", "análise", "contexto", "implicações", "debate", "desafios", "impactos"
)
# Função para limpar o texto
limpar_texto <- function(texto) {
# Remove caracteres especiais, pontuações, números e converte para minúsculas
texto_limpo <- str_to_lower(str_replace_all(texto, "[^[:alnum:][:space:]]", ""))
texto_limpo <- str_squish(texto_limpo)  # Remover espaços extras
return(texto_limpo)
}
# Função para remover stopwords
remover_stopwords <- function(tokens, stopwords) {
# Remove as stopwords
tokens_filtrados <- setdiff(tokens, stopwords)
return(tokens_filtrados)
}
# Função para extrair textos de uma URL que mencionam a cloroquina e covid
extrair_textos_cloroquina <- function(url) {
# Faz o pedido HTTP para a página
resposta <- read_html(url)
# Verifica se a resposta contém conteúdo
if (length(resposta) > 0) {
# Encontra todos os elementos de texto informativo (por exemplo, parágrafos)
textos_informativos <- html_nodes(resposta, "p")
# Extrai, limpa e armazena todos os textos
textos_todos <- sapply(textos_informativos, function(texto) {
texto_limpo <- limpar_texto(html_text(texto))
return(texto_limpo)
})
return(textos_todos)
} else {
cat("Falha ao acessar a página. A resposta não contém conteúdo.\n")
return(NULL)
}
}
# Função para análise de sentimento em relação à covid
analisar_sentimento_covid <- function(texto) {
# Realiza a análise de sentimento
sentimento <- sentiment(texto)
# Verifica se a palavra "covid" está presente na frase
if ("covid" %in% str_split(texto, " ")[[1]]) {
rotulo <- "Positivo"
} else {
rotulo <- "Negativo"
}
return(rotulo)
}
# Lista de URLs para análise
urls <- c(
"https://diplomatique.org.br/do-bolsodoria-ao-bolsonarovirus-o-discurso-de-joao-doria/",
"https://agenciamural.org.br/panorama-da-covid-19-na-grande-sao-paulo/",
"https://jornal.usp.br/ciencias/bolhas-de-protecao-local-podem-ter-freado-covid-19-em-sao-paulo-aponta-pesquisa/",
"https://vejasp.abril.com.br/saude/sao-paulo-900-mil-casos-covid-19?utm_source=google&utm_medium=cpc&utm_campaign=eda_vejasp_audiencia_institucional&gad_source=1&gclid=Cj0KCQiA6vaqBhCbARIsACF9M6nG3mRC82T1K2T4jKti86xQqVQQW2ToCM5NZhw5ORagaiFKIQzqCuMaAsoBEALw_wcB",
"https://www.bbc.com/portuguese/brasil-51746662",
"https://www.bbc.com/portuguese/brasil-53588285",
"https://brasil.elpais.com/brasil/2020-09-11/sao-paulo-puxa-queda-de-mortes-por-covid-19-no-pais-mas-e-cedo-para-cravar-controle-da-pandemia.html",
"https://www.brasildefato.com.br/2020/06/10/autoridades-estao-mandando-populacao-para-abatedouro-com-reabertura-comercial-em-sp",
"https://www.brasildefato.com.br/2020/06/10/autoridades-estao-mandando-populacao-para-abatedouro-com-reabertura-comercial-em-sp",
"https://www.brasildefato.com.br/2020/07/09/vitimas-da-flexibilizacao-relatos-de-quem-contraiu-covid-na-volta-ao-trabalho-em-sp"
)
# Itera sobre a lista de URLs e aplica o web scraping para cada uma delas
todos_textos <- urls %>%
lapply(extrair_textos_cloroquina) %>%
unlist()
library(rvest)
library(sentimentr)
library(tm)
library(dplyr)
# Lista de stopwords em português
stopwords_pt <- c("de", "a", "o", "que", "e", "é", "do", "da", "em", "um", "para", "com", "não", "uma", "os", "no", "se", "na", "por", "mais", "as", "dos", "como", "mas", "ao", "ele", "das", "à", "seu", "sua", "ou", "quando", "muito", "nos", "já", "eu", "também", "só", "pelo", "pela", "até", "isso", "ela", "entre", "depois", "sem", "mesmo", "aos", "seus", "quem", "nas", "me", "esse", "eles", "você", "essa", "num", "nem", "suas", "meu", "às", "minha", "numa", "pelos", "elas", "qual", "nós", "lhe", "deles", "essas", "esses", "pelas", "este", "dele", "tu", "te", "vocês", "vos", "lhes", "meus", "minhas", "teu", "tua", "teus", "tuas", "nosso", "nossa", "nossos", "nossas", "dela", "delas", "esta", "estes", "estas", "aquele", "aquela", "aqueles", "aquelas", "isto", "aquilo", "estou", "está", "estamos", "estão", "estive", "esteve", "estivemos", "estiveram", "estava", "estávamos", "estavam", "estivera", "estivéramos", "esteja", "estejamos", "estejam", "estivesse", "estivéssemos", "estivessem", "estiver", "estivermos", "estiverem", "hei", "há", "havemos", "hão", "houve", "houvemos", "houveram", "houvera", "houvéramos", "haja", "hajamos", "hajam", "houvesse", "houvéssemos", "houvessem", "houver", "houvermos", "houverem", "houverei", "houverá", "houveremos", "houverão", "houveria", "houveríamos", "houveriam", "sou", "somos", "são", "era", "éramos", "eram", "fui", "foi", "fomos", "foram", "fora", "fôramos", "seja", "sejamos", "sejam", "fosse", "fôssemos", "fossem", "for", "formos", "forem", "serei", "será", "seremos", "serão", "seria", "seríamos", "seriam", "tenho", "tem", "temos", "tém", "tinha", "tínhamos", "tinham", "tive", "teve", "tivemos", "tiveram", "tivera", "tivéramos", "tenha", "tenhamos", "tenham", "tivesse", "tivéssemos", "tivessem", "tiver", "tivermos", "tiverem", "faz")
# Palavras adicionais para remover
palavras_a_remover <- c("saúde", "política", "outrotermo", "maisumtermo")
# Palavras personalizadas para aprimorar a análise
stopwords_personalizadas <- c(
"covid", "pandemia", "saúde", "política", "isolamento", "coronavírus",
"vírus", "governo", "casos", "medidas", "quarentena", "vacin", "vacinação",
"contágio", "público", "privado", "impacto", "social", "econômico",
"pandêmico", "lockdown", "epidemia", "trabalho", "economia", "máscara",
"distanciamento", "números", "infectados", "mortes", "vacinas", "imunização",
"doença", "tratamento", "hospital", "prevenção", "transmissão", "isolamento",
"medicamentos", "sintomas", "pesquisa", "científico", "comunidade", "crise",
"precauções", "impacto", "medidas", "segurança", "isolamento", "flexibilização",
"notícias", "informações", "dados", "cenário", "atualização", "sars-cov-2",
"gráfico", "recuperação", "teste", "testagem", "laboratório", "vacinação",
"variantes", "eficácia", "fronteiras", "população", "unidades", "sintomas",
"transparência", "trabalhadores", "grupos", "sociedade", "impactos", "medidas",
"vacinação", "vacinas", "prevenção", "isolamento", "tratamento", "fases",
"protocolos", "especialistas", "relatório", "estudo", "pesquisa", "monitoramento",
"situação", "protocolos", "isolamento", "transparência", "informações", "notícias",
"opiniões", "especialistas", "opinião", "debate", "avaliação", "perspectiva",
"cenário", "análise", "contexto", "implicações", "debate", "desafios", "impactos"
)
# Função para limpar o texto
limpar_texto <- function(texto) {
# Remove caracteres especiais, pontuações, números e converte para minúsculas
texto_limpo <- str_to_lower(str_replace_all(texto, "[^[:alnum:][:space:]]", ""))
texto_limpo <- str_squish(texto_limpo)  # Remover espaços extras
return(texto_limpo)
}
# Função para remover stopwords
remover_stopwords <- function(tokens, stopwords) {
# Remove as stopwords
tokens_filtrados <- setdiff(tokens, stopwords)
return(tokens_filtrados)
}
# Função para extrair textos de uma URL que mencionam a cloroquina e covid
extrair_textos_cloroquina <- function(url) {
# Faz o pedido HTTP para a página
resposta <- read_html(url)
# Verifica se a resposta contém conteúdo
if (length(resposta) > 0) {
# Encontra todos os elementos de texto informativo (por exemplo, parágrafos)
textos_informativos <- html_nodes(resposta, "p")
# Extrai, limpa e armazena todos os textos
textos_todos <- sapply(textos_informativos, function(texto) {
texto_limpo <- limpar_texto(html_text(texto))
return(texto_limpo)
})
return(textos_todos)
} else {
cat("Falha ao acessar a página. A resposta não contém conteúdo.\n")
return(NULL)
}
}
# Função para análise de sentimento em relação à covid
analisar_sentimento_covid <- function(texto) {
# Realiza a análise de sentimento
sentimento <- sentiment(texto)
# Verifica se a palavra "covid" está presente na frase
if ("covid" %in% str_split(texto, " ")[[1]]) {
rotulo <- "Positivo"
} else {
rotulo <- "Negativo"
}
return(rotulo)
}
# Lista de URLs para análise
urls <- c(
"https://diplomatique.org.br/do-bolsodoria-ao-bolsonarovirus-o-discurso-de-joao-doria/",
"https://agenciamural.org.br/panorama-da-covid-19-na-grande-sao-paulo/",
"https://jornal.usp.br/ciencias/bolhas-de-protecao-local-podem-ter-freado-covid-19-em-sao-paulo-aponta-pesquisa/",
"https://vejasp.abril.com.br/saude/sao-paulo-900-mil-casos-covid-19?utm_source=google&utm_medium=cpc&utm_campaign=eda_vejasp_audiencia_institucional&gad_source=1&gclid=Cj0KCQiA6vaqBhCbARIsACF9M6nG3mRC82T1K2T4jKti86xQqVQQW2ToCM5NZhw5ORagaiFKIQzqCuMaAsoBEALw_wcB",
"https://www.bbc.com/portuguese/brasil-51746662",
"https://www.bbc.com/portuguese/brasil-53588285",
"https://brasil.elpais.com/brasil/2020-09-11/sao-paulo-puxa-queda-de-mortes-por-covid-19-no-pais-mas-e-cedo-para-cravar-controle-da-pandemia.html",
"https://www.brasildefato.com.br/2020/06/10/autoridades-estao-mandando-populacao-para-abatedouro-com-reabertura-comercial-em-sp",
"https://www.brasildefato.com.br/2020/06/10/autoridades-estao-mandando-populacao-para-abatedouro-com-reabertura-comercial-em-sp",
"https://www.brasildefato.com.br/2020/07/09/vitimas-da-flexibilizacao-relatos-de-quem-contraiu-covid-na-volta-ao-trabalho-em-sp"
)
# Itera sobre a lista de URLs e aplica o web scraping para cada uma delas
todos_textos <- urls %>%
lapply(extrair_textos_cloroquina) %>%
unlist()
library(rvest)
library(sentimentr)
library(tm)
library(dplyr)
library(stringr)
# Lista de stopwords em português
stopwords_pt <- c("de", "a", "o", "que", "e", "é", "do", "da", "em", "um", "para", "com", "não", "uma", "os", "no", "se", "na", "por", "mais", "as", "dos", "como", "mas", "ao", "ele", "das", "à", "seu", "sua", "ou", "quando", "muito", "nos", "já", "eu", "também", "só", "pelo", "pela", "até", "isso", "ela", "entre", "depois", "sem", "mesmo", "aos", "seus", "quem", "nas", "me", "esse", "eles", "você", "essa", "num", "nem", "suas", "meu", "às", "minha", "numa", "pelos", "elas", "qual", "nós", "lhe", "deles", "essas", "esses", "pelas", "este", "dele", "tu", "te", "vocês", "vos", "lhes", "meus", "minhas", "teu", "tua", "teus", "tuas", "nosso", "nossa", "nossos", "nossas", "dela", "delas", "esta", "estes", "estas", "aquele", "aquela", "aqueles", "aquelas", "isto", "aquilo", "estou", "está", "estamos", "estão", "estive", "esteve", "estivemos", "estiveram", "estava", "estávamos", "estavam", "estivera", "estivéramos", "esteja", "estejamos", "estejam", "estivesse", "estivéssemos", "estivessem", "estiver", "estivermos", "estiverem", "hei", "há", "havemos", "hão", "houve", "houvemos", "houveram", "houvera", "houvéramos", "haja", "hajamos", "hajam", "houvesse", "houvéssemos", "houvessem", "houver", "houvermos", "houverem", "houverei", "houverá", "houveremos", "houverão", "houveria", "houveríamos", "houveriam", "sou", "somos", "são", "era", "éramos", "eram", "fui", "foi", "fomos", "foram", "fora", "fôramos", "seja", "sejamos", "sejam", "fosse", "fôssemos", "fossem", "for", "formos", "forem", "serei", "será", "seremos", "serão", "seria", "seríamos", "seriam", "tenho", "tem", "temos", "tém", "tinha", "tínhamos", "tinham", "tive", "teve", "tivemos", "tiveram", "tivera", "tivéramos", "tenha", "tenhamos", "tenham", "tivesse", "tivéssemos", "tivessem", "tiver", "tivermos", "tiverem", "faz")
# Palavras adicionais para remover
palavras_a_remover <- c("saúde", "política", "outrotermo", "maisumtermo")
# Palavras personalizadas para aprimorar a análise
stopwords_personalizadas <- c(
"covid", "pandemia", "saúde", "política", "isolamento", "coronavírus",
"vírus", "governo", "casos", "medidas", "quarentena", "vacin", "vacinação",
"contágio", "público", "privado", "impacto", "social", "econômico",
"pandêmico", "lockdown", "epidemia", "trabalho", "economia", "máscara",
"distanciamento", "números", "infectados", "mortes", "vacinas", "imunização",
"doença", "tratamento", "hospital", "prevenção", "transmissão", "isolamento",
"medicamentos", "sintomas", "pesquisa", "científico", "comunidade", "crise",
"precauções", "impacto", "medidas", "segurança", "isolamento", "flexibilização",
"notícias", "informações", "dados", "cenário", "atualização", "sars-cov-2",
"gráfico", "recuperação", "teste", "testagem", "laboratório", "vacinação",
"variantes", "eficácia", "fronteiras", "população", "unidades", "sintomas",
"transparência", "trabalhadores", "grupos", "sociedade", "impactos", "medidas",
"vacinação", "vacinas", "prevenção", "isolamento", "tratamento", "fases",
"protocolos", "especialistas", "relatório", "estudo", "pesquisa", "monitoramento",
"situação", "protocolos", "isolamento", "transparência", "informações", "notícias",
"opiniões", "especialistas", "opinião", "debate", "avaliação", "perspectiva",
"cenário", "análise", "contexto", "implicações", "debate", "desafios", "impactos"
)
# Função para limpar o texto
limpar_texto <- function(texto) {
# Remove caracteres especiais, pontuações, números e converte para minúsculas
texto_limpo <- str_to_lower(str_replace_all(texto, "[^[:alnum:][:space:]]", ""))
texto_limpo <- str_squish(texto_limpo)  # Remover espaços extras
return(texto_limpo)
}
# Função para remover stopwords
remover_stopwords <- function(tokens, stopwords) {
# Remove as stopwords
tokens_filtrados <- setdiff(tokens, stopwords)
return(tokens_filtrados)
}
# Função para extrair textos de uma URL que mencionam a cloroquina e covid
extrair_textos_cloroquina <- function(url) {
# Faz o pedido HTTP para a página
resposta <- read_html(url)
# Verifica se a resposta contém conteúdo
if (length(resposta) > 0) {
# Encontra todos os elementos de texto informativo (por exemplo, parágrafos)
textos_informativos <- html_nodes(resposta, "p")
# Extrai, limpa e armazena todos os textos
textos_todos <- sapply(textos_informativos, function(texto) {
texto_limpo <- limpar_texto(html_text(texto))
return(texto_limpo)
})
return(textos_todos)
} else {
cat("Falha ao acessar a página. A resposta não contém conteúdo.\n")
return(NULL)
}
}
# Função para análise de sentimento em relação à covid
analisar_sentimento_covid <- function(texto) {
# Realiza a análise de sentimento
sentimento <- sentiment(texto)
# Verifica se a palavra "covid" está presente na frase
if ("covid" %in% str_split(texto, " ")[[1]]) {
rotulo <- "Positivo"
} else {
rotulo <- "Negativo"
}
return(rotulo)
}
# Lista de URLs para análise
urls <- c(
"https://diplomatique.org.br/do-bolsodoria-ao-bolsonarovirus-o-discurso-de-joao-doria/",
"https://agenciamural.org.br/panorama-da-covid-19-na-grande-sao-paulo/",
"https://jornal.usp.br/ciencias/bolhas-de-protecao-local-podem-ter-freado-covid-19-em-sao-paulo-aponta-pesquisa/",
"https://vejasp.abril.com.br/saude/sao-paulo-900-mil-casos-covid-19?utm_source=google&utm_medium=cpc&utm_campaign=eda_vejasp_audiencia_institucional&gad_source=1&gclid=Cj0KCQiA6vaqBhCbARIsACF9M6nG3mRC82T1K2T4jKti86xQqVQQW2ToCM5NZhw5ORagaiFKIQzqCuMaAsoBEALw_wcB",
"https://www.bbc.com/portuguese/brasil-51746662",
"https://www.bbc.com/portuguese/brasil-53588285",
"https://brasil.elpais.com/brasil/2020-09-11/sao-paulo-puxa-queda-de-mortes-por-covid-19-no-pais-mas-e-cedo-para-cravar-controle-da-pandemia.html",
"https://www.brasildefato.com.br/2020/06/10/autoridades-estao-mandando-populacao-para-abatedouro-com-reabertura-comercial-em-sp",
"https://www.brasildefato.com.br/2020/06/10/autoridades-estao-mandando-populacao-para-abatedouro-com-reabertura-comercial-em-sp",
"https://www.brasildefato.com.br/2020/07/09/vitimas-da-flexibilizacao-relatos-de-quem-contraiu-covid-na-volta-ao-trabalho-em-sp"
)
# Itera sobre a lista de URLs e aplica o web scraping para cada uma delas
todos_textos <- urls %>%
lapply(extrair_textos_cloroquina) %>%
unlist()
# Filtra apenas as frases que mencionam "covid"
textos_covid <- todos_textos[grep("covid", todos_textos, ignore.case = TRUE)]
# Criar um vetor para armazenar os rótulos de sentimento em relação à covid
rotulos_sentimento_covid <- sapply(textos_covid, analisar_sentimento_covid)
# Criar um data frame com os resultados em relação à covid
dados_sentimento_covid <- data.frame(Frase = textos_covid, Sentimento = rotulos_sentimento_covid)
# Contadores de Positivo e Negativo
contador_positivo <- sum(dados_sentimento_covid$Sentimento == "Positivo")
contador_negativo <- sum(dados_sentimento_covid$Sentimento == "Negativo")
# Exibir os resultados
print(dados_sentimento_covid)
cat("\nContador Positivo:", contador_positivo, "\n")
cat("Contador Negativo:", contador_negativo, "\n")
View(dados_sentimento_covid)
library(rvest)
library(sentimentr)
library(tm)
library(dplyr)
library(stringr)
# Lista de stopwords em português
stopwords_pt <- c("de", "a", "o", "que", "e", "é", "do", "da", "em", "um", "para", "com", "não", "uma", "os", "no", "se", "na", "por", "mais", "as", "dos", "como", "mas", "ao", "ele", "das", "à", "seu", "sua", "ou", "quando", "muito", "nos", "já", "eu", "também", "só", "pelo", "pela", "até", "isso", "ela", "entre", "depois", "sem", "mesmo", "aos", "seus", "quem", "nas", "me", "esse", "eles", "você", "essa", "num", "nem", "suas", "meu", "às", "minha", "numa", "pelos", "elas", "qual", "nós", "lhe", "deles", "essas", "esses", "pelas", "este", "dele", "tu", "te", "vocês", "vos", "lhes", "meus", "minhas", "teu", "tua", "teus", "tuas", "nosso", "nossa", "nossos", "nossas", "dela", "delas", "esta", "estes", "estas", "aquele", "aquela", "aqueles", "aquelas", "isto", "aquilo", "estou", "está", "estamos", "estão", "estive", "esteve", "estivemos", "estiveram", "estava", "estávamos", "estavam", "estivera", "estivéramos", "esteja", "estejamos", "estejam", "estivesse", "estivéssemos", "estivessem", "estiver", "estivermos", "estiverem", "hei", "há", "havemos", "hão", "houve", "houvemos", "houveram", "houvera", "houvéramos", "haja", "hajamos", "hajam", "houvesse", "houvéssemos", "houvessem", "houver", "houvermos", "houverem", "houverei", "houverá", "houveremos", "houverão", "houveria", "houveríamos", "houveriam", "sou", "somos", "são", "era", "éramos", "eram", "fui", "foi", "fomos", "foram", "fora", "fôramos", "seja", "sejamos", "sejam", "fosse", "fôssemos", "fossem", "for", "formos", "forem", "serei", "será", "seremos", "serão", "seria", "seríamos", "seriam", "tenho", "tem", "temos", "tém", "tinha", "tínhamos", "tinham", "tive", "teve", "tivemos", "tiveram", "tivera", "tivéramos", "tenha", "tenhamos", "tenham", "tivesse", "tivéssemos", "tivessem", "tiver", "tivermos", "tiverem", "faz")
# Palavras adicionais para remover
palavras_a_remover <- c("saúde", "política", "outrotermo", "maisumtermo")
# Palavras personalizadas para aprimorar a análise
stopwords_personalizadas <- c(
"covid", "pandemia", "saúde", "política", "isolamento", "coronavírus",
"vírus", "governo", "casos", "medidas", "quarentena", "vacin", "vacinação",
"contágio", "público", "privado", "impacto", "social", "econômico",
"pandêmico", "lockdown", "epidemia", "trabalho", "economia", "máscara",
"distanciamento", "números", "infectados", "mortes", "vacinas", "imunização",
"doença", "tratamento", "hospital", "prevenção", "transmissão", "isolamento",
"medicamentos", "sintomas", "pesquisa", "científico", "comunidade", "crise",
"precauções", "impacto", "medidas", "segurança", "isolamento", "flexibilização",
"notícias", "informações", "dados", "cenário", "atualização", "sars-cov-2",
"gráfico", "recuperação", "teste", "testagem", "laboratório", "vacinação",
"variantes", "eficácia", "fronteiras", "população", "unidades", "sintomas",
"transparência", "trabalhadores", "grupos", "sociedade", "impactos", "medidas",
"vacinação", "vacinas", "prevenção", "isolamento", "tratamento", "fases",
"protocolos", "especialistas", "relatório", "estudo", "pesquisa", "monitoramento",
"situação", "protocolos", "isolamento", "transparência", "informações", "notícias",
"opiniões", "especialistas", "opinião", "debate", "avaliação", "perspectiva",
"cenário", "análise", "contexto", "implicações", "debate", "desafios", "impactos"
)
# Função para extrair textos de uma URL que mencionam a cloroquina e covid
extrair_textos_cloroquina <- function(url) {
# Faz o pedido HTTP para a página
resposta <- read_html(url)
# Verifica se a resposta contém conteúdo
if (length(resposta) > 0) {
# Encontra todos os elementos de texto informativo (por exemplo, parágrafos)
textos_informativos <- html_nodes(resposta, "p")
# Extrai, limpa e armazena todos os textos
textos_todos <- sapply(textos_informativos, function(texto) {
texto_limpo <- limpar_texto(html_text(texto))
return(texto_limpo)
})
return(textos_todos)
} else {
cat("Falha ao acessar a página. A resposta não contém conteúdo.\n")
return(NULL)
}
}
# Lista de URLs para análise
urls <- c(
"https://diplomatique.org.br/do-bolsodoria-ao-bolsonarovirus-o-discurso-de-joao-doria/",
"https://agenciamural.org.br/panorama-da-covid-19-na-grande-sao-paulo/",
"https://jornal.usp.br/ciencias/bolhas-de-protecao-local-podem-ter-freado-covid-19-em-sao-paulo-aponta-pesquisa/",
"https://vejasp.abril.com.br/saude/sao-paulo-900-mil-casos-covid-19?utm_source=google&utm_medium=cpc&utm_campaign=eda_vejasp_audiencia_institucional&gad_source=1&gclid=Cj0KCQiA6vaqBhCbARIsACF9M6nG3mRC82T1K2T4jKti86xQqVQQW2ToCM5NZhw5ORagaiFKIQzqCuMaAsoBEALw_wcB",
"https://www.bbc.com/portuguese/brasil-51746662",
"https://www.bbc.com/portuguese/brasil-53588285",
"https://brasil.elpais.com/brasil/2020-09-11/sao-paulo-puxa-queda-de-mortes-por-covid-19-no-pais-mas-e-cedo-para-cravar-controle-da-pandemia.html",
"https://www.brasildefato.com.br/2020/06/10/autoridades-estao-mandando-populacao-para-abatedouro-com-reabertura-comercial-em-sp",
"https://www.brasildefato.com.br/2020/06/10/autoridades-estao-mandando-populacao-para-abatedouro-com-reabertura-comercial-em-sp",
"https://www.brasildefato.com.br/2020/07/09/vitimas-da-flexibilizacao-relatos-de-quem-contraiu-covid-na-volta-ao-trabalho-em-sp"
)
# Itera sobre a lista de URLs e aplica o web scraping para cada uma delas
todos_textos <- urls %>%
lapply(extrair_textos_cloroquina) %>%
unlist()
# Criar um corpus com os textos extraídos
corpus <- Corpus(VectorSource(todos_textos))
# Pré-processar o corpus (remover pontuações, números, converter para minúsculas, etc.)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("portuguese"))
corpus <- tm_map(corpus, removeWords, palavras_a_remover)
corpus <- tm_map(corpus, removeWords, stopwords_personalizadas)
# Filtra apenas as frases que mencionam "covid"
textos_covid <- todos_textos[grep("covid", todos_textos, ignore.case = TRUE)]
# Criar um vetor para armazenar os rótulos de sentimento em relação à covid
rotulos_sentimento_covid <- sapply(textos_covid, analisar_sentimento_covid)
# Criar um data frame com os resultados em relação à covid
dados_sentimento_covid <- data.frame(Frase = textos_covid, Sentimento = rotulos_sentimento_covid)
# Contadores de Positivo e Negativo
contador_positivo <- sum(dados_sentimento_covid$Sentimento == "Positivo")
contador_negativo <- sum(dados_sentimento_covid$Sentimento == "Negativo")
# Exibir os resultados
print(dados_sentimento_covid)
cat("\nContador Positivo:", contador_positivo, "\n")
cat("Contador Negativo:", contador_negativo, "\n")
n")
#setando o nosso repositorio
setwd("/Users/wisley/Documents/Matéria do 4 periodo /Minereção de Texto /Trabalho Final /Minerao-De-Texto--Covid-19")
Arquivo_Noticias <- read.csv2("noticias_categorizada.xlsx")
list.files()
# Instale o pacote se ainda não o fez
install.packages("readxl")
# Carregue o pacote
library(readxl)
# Leia o arquivo Excel
Arquivo_Noticias <- read_excel("noticias_categorizada.xlsx")
# Instale o pacote se ainda não o fez
# Carregue o pacote
library(readxl)
# Leia o arquivo Excel
Arquivo_Noticias <- read_excel("noticias_categorizada.xlsx")
# Instale o pacote se ainda não o fez
# Carregue o pacote
library(readxl)
# Leia o arquivo Excel
arquivo_path <- "/Users/wisley/Documents/Matéria do 4 periodo /Minereção de Texto /Trabalho Final /Minerao-De-Texto--Covid-19/noticias_categorizada.xlsx"
Arquivo_Noticias <- read_excel(arquivo_path)
# Instale o pacote se ainda não o fez
# Carregue o pacote
library(readxl)
# Leia o arquivo Excel
arquivo_path <- "/Users/wisley/Documents/Matéria do 4 periodo /Minereção de Texto /Trabalho Final /Minerao-De-Texto--Covid-19/noticias_categorizada.xlsx"
Arquivo_Noticias <- read_excel(arquivo_path)
# Instale o pacote se ainda não o fez
# Carregue o pacote
library(readxl)
# Leia o arquivo Excel
arquivo_path <- "\Users\wisley\Documents\Matéria do 4 periodo \Minereção de Texto \Trabalho Final \Minerao-De-Texto--Covid-19\noticias_categorizada.xlsx"
# Instale o pacote se ainda não o fez
# Carregue o pacote
library(readxl)
# Leia o arquivo Excel
arquivo_path <- "C:\\Users\\wisley\\Documents\\Matéria do 4 período\\Mineração de Texto\\Trabalho Final\\Mineração-De-Texto--Covid-19\\noticias_categorizada.xlsx"
Arquivo_Noticias <- read_excel(arquivo_path)
# Instale o pacote se ainda não o fez
# Carregue o pacote
library(readxl)
# Leia o arquivo Excel
arquivo_path <- "/Users/wisley/Documents/Matéria do 4 período/Mineração de Texto/Trabalho Final/Mineração-De-Texto--Covid-19/noticias_categorizada.xlsx"
Arquivo_Noticias <- read_excel(arquivo_path)
# Instale o pacote se ainda não o fez
# Carregue o pacote
library(readxl)
# Leia o arquivo Excel
arquivo_path <- "/Users/wisley/Documents/Matéria do 4 periodo /Minereção de Texto /Trabalho Final /Minerao-De-Texto--Covid-19/noticias_categorizadas.xlsx"
Arquivo_Noticias <- read_excel(arquivo_path)
View(Arquivo_Noticias)
# Instale e carregue os pacotes
install.packages(c("tm", "stringr", "dplyr"))
library(tm)
library(stringr)
library(dplyr)
dtm <- DocumentTermMatrix(corpus)
# Instale o pacote se ainda não o fez
# Carregue o pacote
library(readxl)
# Leia o arquivo Excel
arquivo_path <- "/Users/wisley/Documents/Matéria do 4 periodo /Minereção de Texto /Trabalho Final /Minerao-De-Texto--Covid-19/noticias_categorizadas.xlsx"
Arquivo_Noticias <- read_excel(arquivo_path)
# Instale e carregue os pacotes
library(tm)
library(stringr)
library(dplyr)
# Supondo que suas colunas de texto são 'URL', 'Covid', 'Cloroquina', e 'Hidroxicloroquina'
corpus <- Corpus(VectorSource(paste(dados$URL, dados$Covid, dados$Cloroquina, dados$Hidroxicloroquina, sep = " ")))
# Instale o pacote se ainda não o fez
# Carregue o pacote
library(readxl)
# Leia o arquivo Excel
dados <- "/Users/wisley/Documents/Matéria do 4 periodo /Minereção de Texto /Trabalho Final /Minerao-De-Texto--Covid-19/noticias_categorizadas.xlsx"
Arquivo_Noticias <- read_excel(arquivo_path)
# Instale e carregue os pacotes
library(tm)
library(stringr)
library(dplyr)
# Supondo que suas colunas de texto são 'URL', 'Covid', 'Cloroquina', e 'Hidroxicloroquina'
corpus <- Corpus(VectorSource(paste(dados$URL, dados$Covid, dados$Cloroquina, dados$Hidroxicloroquina, sep = " ")))
# Crie um corpus usando suas colunas de texto
corpus <- Corpus(VectorSource(paste(Arquivo_Noticias$URL, Arquivo_Noticias$Covid, Arquivo_Noticias$Cloroquina, Arquivo_Noticias$Hidroxicloroquina, sep = " ")))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
# Se estiver trabalhando com textos em português, use as stopwords em português
stopwords_pt <- stopwords("portuguese")
# Aplique a remoção de stopwords
corpus <- tm_map(corpus, removeWords, stopwords_pt)
dtm <- DocumentTermMatrix(corpus)
View(Arquivo_Noticias)
View(corpus)
View(dtm)
View(dados_sentimento_covid)
