---
title: "Analise de Sentimento"
output: html_notebook
---



```{r}

library(rvest)
library(sentimentr)
library(tm)
library(dplyr)

# Lista de stopwords em português
stopwords_pt <- c("de", "a", "o", "que", "e", "é", "do", "da", "em", "um", "para", "com", "não", "uma", "os", "no", "se", "na", "por", "mais", "as", "dos", "como", "mas", "ao", "ele", "das", "à", "seu", "sua", "ou", "quando", "muito", "nos", "já", "eu", "também", "só", "pelo", "pela", "até", "isso", "ela", "entre", "depois", "sem", "mesmo", "aos", "seus", "quem", "nas", "me", "esse", "eles", "você", "essa", "num", "nem", "suas", "meu", "às", "minha", "numa", "pelos", "elas", "qual", "nós", "lhe", "deles", "essas", "esses", "pelas", "este", "dele", "tu", "te", "vocês", "vos", "lhes", "meus", "minhas", "teu", "tua", "teus", "tuas", "nosso", "nossa", "nossos", "nossas", "dela", "delas", "esta", "estes", "estas", "aquele", "aquela", "aqueles", "aquelas", "isto", "aquilo", "estou", "está", "estamos", "estão", "estive", "esteve", "estivemos", "estiveram", "estava", "estávamos", "estavam", "estivera", "estivéramos", "esteja", "estejamos", "estejam", "estivesse", "estivéssemos", "estivessem", "estiver", "estivermos", "estiverem", "hei", "há", "havemos", "hão", "houve", "houvemos", "houveram", "houvera", "houvéramos", "haja", "hajamos", "hajam", "houvesse", "houvéssemos", "houvessem", "houver", "houvermos", "houverem", "houverei", "houverá", "houveremos", "houverão", "houveria", "houveríamos", "houveriam", "sou", "somos", "são", "era", "éramos", "eram", "fui", "foi", "fomos", "foram", "fora", "fôramos", "seja", "sejamos", "sejam", "fosse", "fôssemos", "fossem", "for", "formos", "forem", "serei", "será", "seremos", "serão", "seria", "seríamos", "seriam", "tenho", "tem", "temos", "tém", "tinha", "tínhamos", "tinham", "tive", "teve", "tivemos", "tiveram", "tivera", "tivéramos", "tenha", "tenhamos", "tenham", "tivesse", "tivéssemos", "tivessem", "tiver", "tivermos", "tiverem", "faz")

# Palavras adicionais para remover
palavras_a_remover <- c("saúde", "política", "outrotermo", "maisumtermo")

# Palavras personalizadas para aprimorar a análise
stopwords_personalizadas <- c(
  "covid", "pandemia", "saúde", "política", "isolamento", "coronavírus",
  "vírus", "governo", "casos", "medidas", "quarentena", "vacin", "vacinação",
  "contágio", "público", "privado", "impacto", "social", "econômico",
  "pandêmico", "lockdown", "epidemia", "trabalho", "economia", "máscara",
  "distanciamento", "números", "infectados", "mortes", "vacinas", "imunização",
  "doença", "tratamento", "hospital", "prevenção", "transmissão", "isolamento",
  "medicamentos", "sintomas", "pesquisa", "científico", "comunidade", "crise",
  "precauções", "impacto", "medidas", "segurança", "isolamento", "flexibilização",
  "notícias", "informações", "dados", "cenário", "atualização", "sars-cov-2",
  "gráfico", "recuperação", "teste", "testagem", "laboratório", "vacinação",
  "variantes", "eficácia", "fronteiras", "população", "unidades", "sintomas",
  "transparência", "trabalhadores", "grupos", "sociedade", "impactos", "medidas",
  "vacinação", "vacinas", "prevenção", "isolamento", "tratamento", "fases",
  "protocolos", "especialistas", "relatório", "estudo", "pesquisa", "monitoramento",
  "situação", "protocolos", "isolamento", "transparência", "informações", "notícias",
  "opiniões", "especialistas", "opinião", "debate", "avaliação", "perspectiva",
  "cenário", "análise", "contexto", "implicações", "debate", "desafios", "impactos"
)

# Função para limpar o texto
limpar_texto <- function(texto) {
  # Remove caracteres especiais, pontuações, números e converte para minúsculas
  texto_limpo <- str_to_lower(str_replace_all(texto, "[^[:alnum:][:space:]]", ""))
  texto_limpo <- str_squish(texto_limpo)  # Remover espaços extras
  
  return(texto_limpo)
}

# Função para remover stopwords
remover_stopwords <- function(tokens, stopwords_pt) {
  # Remove as stopwords
  tokens_filtrados <- setdiff(tokens, stopwords_pt)
  return(tokens_filtrados)
}

# Função para extrair textos de uma URL que mencionam a cloroquina e covid
extrair_textos_cloroquina <- function(url) {
  # Faz o pedido HTTP para a página
  resposta <- read_html(url)

  # Verifica se a resposta contém conteúdo
  if (length(resposta) > 0) {
    # Encontra todos os elementos de texto informativo (por exemplo, parágrafos)
    textos_informativos <- html_nodes(resposta, "p")

    # Extrai, limpa e armazena todos os textos
    textos_todos <- sapply(textos_informativos, function(texto) {
      texto_limpo <- limpar_texto(html_text(texto))
      return(texto_limpo)
    })
    
    return(textos_todos)
  } else {
    cat("Falha ao acessar a página. A resposta não contém conteúdo.\n")
    return(NULL)
  }
}



# Função para análise de sentimento em relação à covid
analisar_sentimento_covid <- function(texto) {
  # Realiza a análise de sentimento
  sentimento <- sentiment(texto)

  # Verifica se a palavra "covid" está presente na frase
  if ("covid" %in% str_split(texto, " ")[[1]]) {
    rotulo <- "Positivo"
  } else {
    rotulo <- "Negativo"
  }

  return(rotulo)
}
# Lista de URLs para análise
urls <- c(
  "https://diplomatique.org.br/do-bolsodoria-ao-bolsonarovirus-o-discurso-de-joao-doria/",
  "https://agenciamural.org.br/panorama-da-covid-19-na-grande-sao-paulo/",
  "https://jornal.usp.br/ciencias/bolhas-de-protecao-local-podem-ter-freado-covid-19-em-sao-paulo-aponta-pesquisa/",
  "https://vejasp.abril.com.br/saude/sao-paulo-900-mil-casos-covid-19?utm_source=google&utm_medium=cpc&utm_campaign=eda_vejasp_audiencia_institucional&gad_source=1&gclid=Cj0KCQiA6vaqBhCbARIsACF9M6nG3mRC82T1K2T4jKti86xQqVQQW2ToCM5NZhw5ORagaiFKIQzqCuMaAsoBEALw_wcB",
  "https://www.bbc.com/portuguese/brasil-51746662",
  "https://www.bbc.com/portuguese/brasil-53588285",
  "https://brasil.elpais.com/brasil/2020-09-11/sao-paulo-puxa-queda-de-mortes-por-covid-19-no-pais-mas-e-cedo-para-cravar-controle-da-pandemia.html",
  "https://www.brasildefato.com.br/2020/06/10/autoridades-estao-mandando-populacao-para-abatedouro-com-reabertura-comercial-em-sp",
  "https://www.brasildefato.com.br/2020/06/10/autoridades-estao-mandando-populacao-para-abatedouro-com-reabertura-comercial-em-sp",
  "https://www.brasildefato.com.br/2020/07/09/vitimas-da-flexibilizacao-relatos-de-quem-contraiu-covid-na-volta-ao-trabalho-em-sp"
)


# Itera sobre a lista de URLs e aplica o web scraping para cada uma delas
todos_textos <- urls %>%
  lapply(extrair_textos_cloroquina) %>%
  unlist()

# Filtra apenas as frases que mencionam "covid"
textos_covid <- todos_textos[grep("covid", todos_textos, ignore.case = TRUE)]

# Criar um vetor para armazenar os rótulos de sentimento em relação à covid
rotulos_sentimento_covid <- sapply(textos_covid, analisar_sentimento_covid)

# Criar um data frame com os resultados em relação à covid
dados_sentimento_covid <- data.frame(Frase = textos_covid, Sentimento = rotulos_sentimento_covid)

# Contadores de Positivo e Negativo
contador_positivo <- sum(dados_sentimento_covid$Sentimento == "Positivo")
contador_negativo <- sum(dados_sentimento_covid$Sentimento == "Negativo")

# Exibir os resultados
print(dados_sentimento_covid)
cat("\nContador Positivo:", contador_positivo, "\n")
cat("Contador Negativo:", contador_negativo, "\n")


```


